#!/usr/bin/env python3
#
# An OBS Source Service to package the published repository as a tarball file
# and publish it to another server.
# (C) 2021 SUSE LLC
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
# See http://www.gnu.org/licenses/gpl-2.0.html for full license text.
#
"""\
OBS Source Service to package the published repository as a tarball, compressed with
the specified method (default to "gz"), containing the published repository.

See README.md for additional documentation.
"""

import logging
import argparse
import tarfile
import os
from xml.etree.ElementInclude import include
import urllib3
import re
import xml.etree.ElementTree as ET
import glob
from datetime import datetime
from subprocess import check_output
from subprocess import CalledProcessError
import packaging.version as v
import pycurl
import urllib.parse
from io import BytesIO


app_name = "obs-service-publish_tar"

description = __doc__

logging.basicConfig(level=logging.DEBUG)
log = logging.getLogger(app_name)

DEFAULT_COMPRESSION = "gz"

connect_timeout = 20    # max time in seconds for connecting
download_timeout = 20   # max time without receiving data to cancel the download

include_list = []
exclude_list = []

parser = argparse.ArgumentParser(
    description=description, formatter_class=argparse.RawDescriptionHelpFormatter
)
parser.add_argument("--destinationpath", default="srv/www/htdocs")
parser.add_argument("--archive")
parser.add_argument("--outdir")
parser.add_argument("--compression")
parser.add_argument("--changesgenerate", default="disable")
parser.add_argument("--sourceproject")
parser.add_argument("--repository")
parser.add_argument("--publish_url")
parser.add_argument("--obs_server")
parser.add_argument("--obs_user")
parser.add_argument("--obs_password")
parser.add_argument("--keeplatest", default="disable")
parser.add_argument("--extrapackages")

args = parser.parse_args()

destinationpath = args.destinationpath.lstrip('/')
outdir = args.outdir
if args.changesgenerate == "enable":
    changes_generate = True
else:
    changes_generate = False

if args.keeplatest == "enable":
    keeplatest = True
else:
    keeplatest = False

extrapackages = args.extrapackages
sourceproject = args.sourceproject
repository = args.repository
compression = args.compression
publish_url = args.publish_url
archive = args.archive
obs_server = args.obs_server
obs_user = args.obs_user
obs_password = args.obs_password
    
if '%d' in archive:
    archive=archive.replace('%d', datetime.now().strftime('%Y-%m-%d'))
    
log.info(f"Using archive {archive}")
http = urllib3.PoolManager()

def create_exclude_list(repopath):

	regex = r'(.*)-(.*)-(.*?.*)\.(noarch|x86_64|src)\.(rpm|drpm)'

	files=glob.glob(repopath + '/**/*rpm')
	packages = [os.path.basename(x) for x in files]
	include = []
	exclude = []
	versions = {}
	# print(f'total packages: {len(packages)}')
	for item in packages:
		matches = re.search(regex, item)
		package = matches.group(1)
		version = matches.group(2)
		release = matches.group(3)
		if package in versions.keys():
			versions[package].append(v.parse(version + '-' + release))
		else:
			versions[package] = [v.parse(version + '-' + release)]
		versions[package].sort(reverse=True)

	for item in packages:
		matches = re.search(regex, item)
		package = matches.group(1)
		version = matches.group(2)
		release = matches.group(3)
		arch = matches.group(4)
		type = matches.group(5)
		if '.drpm' in item or '.src.rpm' in item:
			# print(f'excluding DRPM/SRC item: {item}')
			exclude.append(item)
		elif item not in exclude and package in versions.keys() and v.parse(version + '-' + release) == versions[package][0]:
			# print(f'including item: {item}')
			include.append(item)
		else:
			# print(f'excluding item: {item}')
			exclude.append(item)

	include.sort()
	exclude.sort()
	log.info(f'----total packages: {len(packages)}')
	log.info(f'----excluded: {len(exclude)}')
	log.info(f'----included: {len(include)}')

	return include, exclude

### reset uid/gid to 0/0 (root).
def reset(tarinfo):
    global exclude_list
    tarinfo.uid = tarinfo.gid = 0
    tarinfo.uname = tarinfo.gname = "root"
    if tarinfo.isreg() and os.path.basename(tarinfo.name) in exclude_list:
        # print(f'excluding {tarinfo.name} from the tarball')
        return None
    return tarinfo

def run_cmd(cmd, dir='.'):
    try:
        log.info(f"executing command: {cmd} on directory {dir}")
        cwd = os.getcwd()
        output = check_output(cmd, cwd=dir, shell=True).decode("utf-8").strip()
        if output:
            log.info(output)
        os.chdir(cwd)

    except CalledProcessError as e:
        error = e.output.decode("utf-8").strip()
        if error:
            log.info(error)
        os.chdir(cwd)
        raise
    
def get_packages(project_name):
    try:
        headers = urllib3.make_headers(basic_auth=obs_user + ':' + obs_password)
        r = http.request('GET', 'https://' + obs_server + '/source/' + project_name, headers=headers)
    except Exception as e:
            raise ConnectionAbortedError('Error connecting to OBS API: ' + str(e))

    packagelist = []
    if r.status == 200:
        tree = ET.XML(r.data)
        for f in tree.findall('entry/[@name]'):
            packagelist.append(f.attrib['name'])
    else:
        raise ConnectionAbortedError('error received from OBS API server: ' + str(r.status))
    return packagelist

def get_filelist(project_name, package_name):
    try:
        headers = urllib3.make_headers(basic_auth=obs_user + ':' + obs_password)
        r = http.request('GET', 'https://' + obs_server + '/source/' + project_name + '/' + package_name, headers=headers)
    except Exception as e:
            raise ConnectionAbortedError('Error connecting to OBS API: ' + str(e))

    filelist = []
    if r.status == 200:
        tree = ET.XML(r.data)
        for f in tree.findall('entry/[@name]'):
            filelist.append(f.attrib['name'])
    else:
        raise ConnectionAbortedError('error received from OBS API server: ' + str(r.status))
    return filelist

def get_file(project_name, package_name, file_name):
    try:
        headers = urllib3.make_headers(basic_auth=obs_user + ':' + obs_password)
        r = http.request('GET', 'https://' + obs_server + '/source/' + project_name + '/' + package_name + '/' + file_name, headers=headers)
    except Exception as e:
            raise ConnectionAbortedError('Error connecting to OBS API: ' + str(e))

    filecontents = ""
    if r.status == 200:
        filecontents = r.data
    else:
        raise ConnectionAbortedError('error received from OBS API server: ' + str(r.status))
    return filecontents.decode('utf-8')

def download_extrapackages(manifest):
    
    return

def generate_changelog(project):
    packages = get_packages(project)
    log.info(f"packages: {packages}")
    
    changelog = ""
    for f in packages:
        files = get_filelist(project, f)
        log.info(f"files for package {f}: {files}")
        for g in files:
            if ".changes" in g:
                filecontent = get_file(project, f, g)
                header = f"\n\n\n*=*=*=*=*=*=*=*=*=*=*=*[ changelog: {f} ]*=*=*=*=*=*=*=*=*=*=*=*\n\n"
                changelog += header
                changelog += f"{filecontent}"
                
    return changelog


def download_url(url, filename=None, spider=False, force=False):
    status_code = -1
    bytes_size = -1
    fd = None
    log.info(f"tipo = {type(url)}")
    try:
        data = BytesIO()
        c = pycurl.Curl()
        c.setopt(pycurl.URL, str(url))
        c.setopt(pycurl.CONNECTTIMEOUT, connect_timeout)            # timeout for connecting
        c.setopt(pycurl.LOW_SPEED_TIME, download_timeout)           # timeout for receiving data
        c.setopt(pycurl.VERBOSE, 0)
        c.setopt(c.NOPROGRESS, 1)                                   # disable progress callbacks
        c.setopt(pycurl.NOBODY,1)                                   # only looks for the size of the file (HEAD)
        c.setopt(pycurl.SSL_VERIFYPEER, 0)                          # do not verify server certificates
        c.setopt(pycurl.SSL_VERIFYHOST, 0)                          # ignore self-signed certificates
        c.perform()
        status_code = c.getinfo(c.RESPONSE_CODE)
        bytes_size = int(c.getinfo(c.CONTENT_LENGTH_DOWNLOAD))
        
        # if "spider" was requested, return now
        if spider: 
            log.info('[spider]: %s  status: %d (%d bytes)' % (url, status_code, bytes_size))
            return status_code, bytes_size
        else:
            c.setopt(pycurl.NOBODY, 0)                              # now we download the full file
            
        if filename is None:
            c.setopt(c.WRITEFUNCTION, data.write)                   # no file, return contents in memory
        else:
            # if the file already exists...
            if os.path.exists(filename):
                file_size = os.path.getsize(filename)
                if (file_size == bytes_size) and force is False:
                    log.info('[OK] file %s was already downloaded completely (%d bytes).' % (url, bytes_size))
                    return status_code, None
                    
            fd = open(filename, 'wb')
            c.setopt(c.WRITEFUNCTION, fd.write)
            data = None

        if status_code == 404:
            log.info('[ERROR] file %s does not exist on the server' % url)
            fd.close()
            os.remove(filename)
            return status_code, None
        
        # do the download
        log.info('* initiating download: %s (%d bytes)' % (url, bytes_size))
        c.perform()
        if fd is not None:
            fd.close()
        total_time = int(c.getinfo(c.TOTAL_TIME))
        c.close()   
        log.info('[OK] download successful: %s status: %d (%d bytes, %s seconds)' % (url, status_code, bytes_size, total_time))
            
    except pycurl.error as e:
        if e.args[0] == pycurl.E_COULDNT_CONNECT:
            errmsg = 'connection error'
        if e.args[0] == pycurl.E_OPERATION_TIMEDOUT:
            errmsg = 'timeout reached.'
        else:
            errmsg = '[ERROR] ' + str(pycurl.error(e))
        log.info(errmsg)
        if fd is not None:
            fd.close()
        return -1, errmsg
    except Exception as e:
        errmsg = '[ERROR] ' + str(e)
        if fd is not None:
            fd.close()
        return -1, errmsg
        
    if status_code == 200 and data is not None:
        return status_code, data.getvalue().decode('utf-8')
    else:
        return status_code, None


def main():
    global include_list
    global exclude_list
    
    # urllib object
    urllib3.disable_warnings()
    
    log.info(f"Running OBS Source Service: {app_name}")

    log.info(f"Destination path: {destinationpath}, outdir: {outdir}")
    log.info(f"Source project: {sourceproject}, generate changes: {changes_generate}")
    log.info(f"Repository: {repository}")
    workdir = outdir + '/../tmp/'
    contentdir = os.path.join(workdir, destinationpath)
    sourcedir = os.path.join('/srv/obs/repos/' + sourceproject.replace(':', ':/'), repository)
    origindir = os.getcwd()
    
    try:
        os.makedirs(workdir, mode=0o775)
        os.chdir(workdir)
        os.makedirs(destinationpath, mode=0o775)
        
        out_file = os.path.join(outdir, archive)

                
        log.info(f"Creating archive as {out_file}, compression is {compression}")
        log.info(f"Files will be taken from {workdir}")
        
        ### generate the changelog
        if changes_generate:
            basename = archive.split('.')[0]
            log.info(f"Creating project changelog")
            changelog = generate_changelog(sourceproject)
            with open(os.path.join(contentdir, basename + '.changelog'), 'w') as f:
                f.write(changelog)
            f.close()
            
            with open(os.path.join(outdir, basename + '.changelog'), 'w') as f:
                f.write(changelog)
            f.close()
        
        try:
            if keeplatest:
                run_cmd(["/bin/cp -av " + sourcedir + "/{noarch,x86_64} ."], contentdir)
            else:
                run_cmd(["/bin/cp -av " + sourcedir + "/{noarch,x86_64,repodata} ."], contentdir)
        except CalledProcessError:
            log.error(f"Error while copying content to {contentdir}")
            exit(1)
            
        ### download extra package URLs, if present
        if extrapackages:
            log.info(f"Downloading files from manifest: {extrapackages}")
            extrapath = os.path.join(contentdir, 'extrapackages')
            os.makedirs(extrapath, mode=0o775)
            with open(os.path.join(origindir, extrapackages), 'r') as f:
                url_list = f.read().splitlines()
            while("" in url_list):
                url_list.remove("")
            if url_list:
                log.info(f"Total URLs to download: {len(url_list)}")
                for u in url_list:
                    log.info(f"Downloading file {u}")
                    status, data = download_url(urllib.parse.quote(u, safe=':/'), filename=os.path.join(extrapath, os.path.basename(u)), force=True)
                    if status != 200:
                        log.error('===> error downloading file %s (%s) : ' % (u, data))
                        exit(1)
                                  
        ### create exclusion lists
        if keeplatest:
            include_list, exclude_list = create_exclude_list(contentdir)
            for item in exclude_list:
                run_cmd(["find " + contentdir +  " -type f -name " + item + " -delete"])
            run_cmd(["rm -rf repodata"], contentdir)
            run_cmd(["/usr/bin/createrepo " + contentdir + " 2>&1"], contentdir)
    
        ### create archive
        os.chdir(workdir)
        with tarfile.open(out_file, 'w:' + compression) as tar_handle:
            tar_handle.add(destinationpath, recursive=True, filter=reset)

        log.info(f"Created archive at {tar_handle.name}")
        tar_handle.close()    
                
        if publish_url:
            try:
                run_cmd(["mkdir -p update-archives"], '/srv/obs/repos')
                run_cmd(["cp -av " + out_file + " .",], '/srv/obs/repos/update-archives/')
                run_cmd(["cp -av " + os.path.join(contentdir, basename + '.changelog') + " .",], '/srv/obs/repos/update-archives/')
            except CalledProcessError:
                log.error(f"Error while copying tarfile to repo")
                exit(1)

            try:            
                run_cmd(['/usr/bin/rsync -ar0 --timeout 7200 /srv/obs/repos/update-archives/ ' + publish_url + '/'], '.')
            except CalledProcessError:
                log.error(f"Error while syncing files to {publish_url}")
                exit(1)
        
    except Exception as e:
        log.error(f"Error while trying to create archive: {e}")
        exit(1)


if __name__ == "__main__":
    main()
