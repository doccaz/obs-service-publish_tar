#!/usr/bin/env python3
#
# An OBS Source Service to package the published repository as a tarball file
# and publish it to another server.
# (C) 2021 SUSE LLC
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
# See http://www.gnu.org/licenses/gpl-2.0.html for full license text.
#
"""\
OBS Source Service to package the published repository as a tarball, compressed with
the specified method (default to "gz"), containing the published repository.

See README.md for additional documentation.
"""

import logging
import argparse
import tarfile
import os
from xml.etree.ElementInclude import include
import urllib3
import re
import xml.etree.ElementTree as ET
import glob
from datetime import datetime
from subprocess import check_output
from subprocess import CalledProcessError
from packaging.version import Version, parse
from distutils.version import LooseVersion
import pycurl
import urllib.parse
import hashlib
from io import BytesIO

app_name = "obs-service-publish_tar"

description = __doc__

logging.basicConfig(level=logging.DEBUG)
log = logging.getLogger(app_name)

DEFAULT_COMPRESSION = "gz"

connect_timeout = 20    # max time in seconds for connecting
download_timeout = 20   # max time without receiving data to cancel the download

include_list = []
exclude_list = []

parser = argparse.ArgumentParser(
    description=description, formatter_class=argparse.RawDescriptionHelpFormatter
)
parser.add_argument("--destinationpath", default="srv/www/htdocs")
parser.add_argument("--archive")
parser.add_argument("--outdir")
parser.add_argument("--compression")
parser.add_argument("--changesgenerate", default="disable")
parser.add_argument("--sourceproject")
parser.add_argument("--repository")
parser.add_argument("--publish_url")
parser.add_argument("--obs_server")
parser.add_argument("--obs_user")
parser.add_argument("--obs_password")
parser.add_argument("--keeplatest", default="disable")
parser.add_argument("--extrapackages")
parser.add_argument("--maxversions")

currentproject = os.getenv('OBS_SERVICE_PROJECT')

args = parser.parse_args()

destinationpath = args.destinationpath.lstrip('/')
outdir = args.outdir
if args.changesgenerate == "enable":
    changes_generate = True
else:
    changes_generate = False

if args.keeplatest == "enable":
    keeplatest = True
else:
    keeplatest = False

extrapackages = args.extrapackages
maxversions = args.maxversions
sourceproject = args.sourceproject
repository = args.repository
compression = args.compression
publish_url = args.publish_url
archive = args.archive
obs_server = args.obs_server
obs_user = args.obs_user
obs_password = args.obs_password
    
if '%d' in archive:
    archive=archive.replace('%d', datetime.now().strftime('%Y-%m-%d'))
    
log.info(f"Using archive {archive}")
http = urllib3.PoolManager()

def create_exclude_list(repopath, maxversions_dict):

    log.info(f"maxversions = {maxversions_dict}")
    log.info(f"Total items in max versions: {len(maxversions_dict)}")

    regex = r'(.*)-(.*)-(.*?.*)\.(noarch|x86_64|src)\.(rpm|drpm)'

    files=glob.glob(repopath + '/**/*rpm')
    packages = [os.path.basename(x) for x in files]
    include = []
    exclude = []
    versions = {}
    # print(f'total packages: {len(packages)}')
    for item in packages:
        matches = re.search(regex, item)
        package = matches.group(1)
        version = matches.group(2)
        release = matches.group(3)
        if package in versions.keys():   
                versions[package].append(parse(version + '-' + release))
        else:
                versions[package] = [parse(version + '-' + release)]
        versions[package].sort(reverse=True)
    
    ## remove any packages that have versions bigger than the ones in maxversions
                # if any(package in s for s in include):
                #     log.info(f'including package {item} (lower than {maxversions_dict[package]})')
                #     include.append(item)

        if package in maxversions_dict.keys() and (LooseVersion(version + '-' + release) >  LooseVersion(maxversions_dict[package])):
            log.info(f'excluding package {package}-{version} (higher than {maxversions_dict[package]})')
            exclude.append(item)    
 

    for item in packages:
        matches = re.search(regex, item)
        package = matches.group(1)
        version = matches.group(2)
        release = matches.group(3)
        arch = matches.group(4)
        type = matches.group(5)
        log.info(f"processing package {package}-{version}")
        if '.drpm' in item or '.src.rpm' in item:
			# print(f'excluding DRPM/SRC item: {item}')
            exclude.append(item)
        elif item not in exclude and package in versions.keys() and parse(version + '-' + release) == versions[package][0]:
			    # print(f'including item: {item}')
                log.info(f'including package {item} (no restriction)')
                include.append(item)
        else:
			# print(f'excluding item: {item}')
            exclude.append(item)

    include.sort()
    exclude.sort()
    log.info(f'----total packages: {len(packages)}')
    log.info(f'----excluded: {len(exclude)}')
    log.info(f'----included: {len(include)}')

    return include, exclude

### reset uid/gid to 0/0 (root).
def reset(tarinfo):
    global exclude_list
    tarinfo.uid = tarinfo.gid = 0
    tarinfo.uname = tarinfo.gname = "root"
    if tarinfo.isreg() and os.path.basename(tarinfo.name) in exclude_list:
        # print(f'excluding {tarinfo.name} from the tarball')
        return None
    return tarinfo

def run_cmd(cmd, dir='.'):
    try:
        log.info(f"executing command: {cmd} on directory {dir}")
        cwd = os.getcwd()
        output = check_output(cmd, cwd=dir, shell=True).decode("utf-8").strip()
        if output:
            log.info(output)
        os.chdir(cwd)
        return output
    except CalledProcessError as e:
        error = e.output.decode("utf-8").strip()
        if error:
            log.info(error)
        os.chdir(cwd)
        raise

def get_publish_url_from_project(project_name):
    try:
        log.info(f'Fetching project config for {project_name}')
        headers = urllib3.make_headers(basic_auth=obs_user + ':' + obs_password)
        r = http.request('GET', 'https://' + obs_server + '/source/' + project_name + '/_config', headers=headers)
    except Exception as e:
            raise ConnectionAbortedError('Error connecting to OBS API: ' + str(e))

    prjconf = None
    if r.status == 200:
        prjconf = r.data.decode('utf8')
        log.info(f'response from server: {prjconf}')
        
    if prjconf:
        regex = r"^%define\s+publish_url\s+(.*$)"
        matches = re.search(regex, prjconf,  re.MULTILINE)
        return matches.group(1)
    
    return None
    
def get_packages(project_name):
    try:
        headers = urllib3.make_headers(basic_auth=obs_user + ':' + obs_password)
        r = http.request('GET', 'https://' + obs_server + '/source/' + project_name, headers=headers)
    except Exception as e:
            raise ConnectionAbortedError('Error connecting to OBS API: ' + str(e))

    packagelist = []
    if r.status == 200:
        tree = ET.XML(r.data)
        for f in tree.findall('entry/[@name]'):
            packagelist.append(f.attrib['name'])
    else:
        raise ConnectionAbortedError('error received from OBS API server: ' + str(r.status))
    return packagelist

def get_filelist(project_name, package_name):
    try:
        headers = urllib3.make_headers(basic_auth=obs_user + ':' + obs_password)
        r = http.request('GET', 'https://' + obs_server + '/source/' + project_name + '/' + package_name, headers=headers)
    except Exception as e:
            raise ConnectionAbortedError('Error connecting to OBS API: ' + str(e))

    filelist = []
    if r.status == 200:
        tree = ET.XML(r.data)
        for f in tree.findall('entry/[@name]'):
            filelist.append(f.attrib['name'])
    else:
        raise ConnectionAbortedError('error received from OBS API server: ' + str(r.status))
    return filelist

def get_file(project_name, package_name, file_name):
    try:
        headers = urllib3.make_headers(basic_auth=obs_user + ':' + obs_password)
        r = http.request('GET', 'https://' + obs_server + '/source/' + project_name + '/' + package_name + '/' + file_name, headers=headers)
    except Exception as e:
            raise ConnectionAbortedError('Error connecting to OBS API: ' + str(e))

    filecontents = ""
    if r.status == 200:
        filecontents = r.data
    else:
        raise ConnectionAbortedError('error received from OBS API server: ' + str(r.status))
    return filecontents.decode('utf-8')


def generate_changelog(project):
    packages = get_packages(project)
    log.info(f"packages: {packages}")
    
    changelog = ""
    for f in packages:
        files = get_filelist(project, f)
        log.info(f"files for package {f}: {files}")
        for g in files:
            if ".changes" in g:
                filecontent = get_file(project, f, g)
                header = f"\n\n\n*=*=*=*=*=*=*=*=*=*=*=*[ changelog: {f} ]*=*=*=*=*=*=*=*=*=*=*=*\n\n"
                changelog += header
                changelog += f"{filecontent}"
                
    return changelog

def generate_changelog_from_repo(repo_dir):
    packages = glob.glob(repo_dir + '/**/*rpm')
    log.info(f"packages: {packages}")
    
    changelog = ""
    for f in packages:
        log.info(f"processing package {f}:")
        header = f"\n\n\n*=*=*=*=*=*=*=*=*=*=*=*[ changelog: {os.path.basename(f)} ]*=*=*=*=*=*=*=*=*=*=*=*\n\n"
        changelog += header
        output=run_cmd(f"rpm -qp --changelog {f}")
        changelog += f"{output}"
                
    return changelog

def calculate_sha1sum(filename):

    sha1sum = hashlib.sha1()
    with open(filename, 'rb') as source:
        block = source.read(2**16)
        while len(block) != 0:
            sha1sum.update(block)
            block = source.read(2**16)

    return sha1sum.hexdigest()


def download_url(url, filename=None, spider=False, force=False):
    status_code = -1
    bytes_size = -1
    fd = None
    log.info(f"tipo = {type(url)}")
    try:
        data = BytesIO()
        c = pycurl.Curl()
        c.setopt(pycurl.URL, str(url))
        c.setopt(pycurl.CONNECTTIMEOUT, connect_timeout)            # timeout for connecting
        c.setopt(pycurl.LOW_SPEED_TIME, download_timeout)           # timeout for receiving data
        c.setopt(pycurl.VERBOSE, 0)
        c.setopt(c.NOPROGRESS, 1)                                   # disable progress callbacks
        c.setopt(pycurl.NOBODY,1)                                   # only looks for the size of the file (HEAD)
        c.setopt(pycurl.SSL_VERIFYPEER, 0)                          # do not verify server certificates
        c.setopt(pycurl.SSL_VERIFYHOST, 0)                          # ignore self-signed certificates
        c.perform()
        status_code = c.getinfo(c.RESPONSE_CODE)
        bytes_size = int(c.getinfo(c.CONTENT_LENGTH_DOWNLOAD))
        
        # if "spider" was requested, return now
        if spider: 
            log.info('[spider]: %s  status: %d (%d bytes)' % (url, status_code, bytes_size))
            return status_code, bytes_size
        else:
            c.setopt(pycurl.NOBODY, 0)                              # now we download the full file
            
        if filename is None:
            c.setopt(c.WRITEFUNCTION, data.write)                   # no file, return contents in memory
        else:
            # if the file already exists...
            if os.path.exists(filename):
                file_size = os.path.getsize(filename)
                if (file_size == bytes_size) and force is False:
                    log.info('[OK] file %s was already downloaded completely (%d bytes).' % (url, bytes_size))
                    return status_code, None
                    
            fd = open(filename, 'wb')
            c.setopt(c.WRITEFUNCTION, fd.write)
            data = None

        if status_code == 404:
            log.info('[ERROR] file %s does not exist on the server' % url)
            fd.close()
            os.remove(filename)
            return status_code, None
        
        # do the download
        log.info('* initiating download: %s (%d bytes)' % (url, bytes_size))
        c.perform()
        if fd is not None:
            fd.close()
        total_time = int(c.getinfo(c.TOTAL_TIME))
        c.close()   
        log.info('[OK] download successful: %s status: %d (%d bytes, %s seconds)' % (url, status_code, bytes_size, total_time))
            
    except pycurl.error as e:
        if e.args[0] == pycurl.E_COULDNT_CONNECT:
            errmsg = 'connection error'
        if e.args[0] == pycurl.E_OPERATION_TIMEDOUT:
            errmsg = 'timeout reached.'
        else:
            errmsg = '[ERROR] ' + str(pycurl.error(e))
        log.info(errmsg)
        if fd is not None:
            fd.close()
        return -1, errmsg
    except Exception as e:
        errmsg = '[ERROR] ' + str(e)
        if fd is not None:
            fd.close()
        return -1, errmsg
        
    if status_code == 200 and data is not None:
        return status_code, data.getvalue().decode('utf-8')
    else:
        return status_code, None


def main():
    global include_list
    global exclude_list
    global publish_url
    
    # urllib object
    urllib3.disable_warnings()
    
    log.info(f"Running OBS Source Service: {app_name}")
    log.info(f"Current project is: {currentproject}")
    log.info(f"Destination path: {destinationpath}, outdir: {outdir}")
    log.info(f"Source project: {sourceproject}, generate changes: {changes_generate}")
    log.info(f"Repository: {repository}")
    
    # try to fetch the publish URL from the project config if it's not defined in the _service
    if publish_url:
        log.info(f"Publish URL is: {publish_url}")
    else:
        publish_url = get_publish_url_from_project(currentproject)
        log.info(f"Publish URL set via project config:")
        
    workdir = outdir + '/../tmp/'
    contentdir = os.path.join(workdir, destinationpath)
    sourcedir = os.path.join('/srv/obs/repos/' + sourceproject.replace(':', ':/'), repository)
    origindir = os.getcwd()
    
    try:
        os.makedirs(workdir, mode=0o775)
        os.chdir(workdir)
        os.makedirs(destinationpath, mode=0o775)
        
        out_file = os.path.join(outdir, archive)

        log.info(f"Creating archive as {out_file}, compression is {compression}")
        log.info(f"Files will be taken from {workdir}")
        
        try:
            if keeplatest:
                run_cmd(["/bin/cp -av " + sourcedir + "/{noarch,x86_64} ."], contentdir)
            else:
                run_cmd(["/bin/cp -av " + sourcedir + "/{noarch,x86_64,repodata} ."], contentdir)
        except CalledProcessError:
            log.error(f"Error while copying content to {contentdir}")
            exit(1)
            
        ### define basename to be used everywhere else
        basename = archive.split('.')[0]
         
        ### parse max versions file, if present
        maxversions_dict = {}
        if maxversions:
            log.info(f"Loading maximum versions list: {maxversions}")
            with open(os.path.join(origindir, maxversions), 'r') as f:
                maxtmp = f.read().splitlines()
                for l in maxtmp:
                    if ',' not in l:
                        maxtmp.remove(l)
                    else:
                        maxversions_dict[l.split(',')[0]] = l.split(',')[1]
            log.info(f"maxversions = {maxversions_dict}")
            log.info(f"Total items in max versions: {len(maxversions_dict)}")
                            
        ### download extra package URLs, if present
        if extrapackages:
            log.info(f"Downloading files from manifest: {extrapackages}")
            extrapath = os.path.join(contentdir, 'extrapackages')
            os.makedirs(extrapath, mode=0o775)
            with open(os.path.join(origindir, extrapackages), 'r') as f:
                url_list = f.read().splitlines()
            while("" in url_list):
                url_list.remove("")
            if url_list:
                log.info(f"Total URLs to download: {len(url_list)}")
                for u in url_list:
                    log.info(f"Downloading file {u}")
                    status, data = download_url(urllib.parse.quote(u, safe=':/'), filename=os.path.join(extrapath, os.path.basename(u)), force=True)
                    if status != 200:
                        log.error('===> error downloading file %s (%s) : ' % (u, data))
                        exit(1)
                    else:
                        log.info('All files downloaded.')
                                  
        ### create exclusion lists
        if keeplatest:
            include_list, exclude_list = create_exclude_list(contentdir, maxversions_dict)
            for item in exclude_list:
                run_cmd(["find " + contentdir +  " -type f -name " + item + " -delete"])
            run_cmd(["rm -rf repodata"], contentdir)
            run_cmd(["/usr/bin/createrepo " + contentdir + " 2>&1"], contentdir)
    
        ### generate the changelog
        if changes_generate: 
            log.info(f"Creating project changelog")
            #changelog = generate_changelog(sourceproject)
            changelog = generate_changelog_from_repo(destinationpath)
            with open(os.path.join(contentdir, basename + '.changelog'), 'w') as f:
                f.write(changelog)
            f.close()
            
            with open(os.path.join(outdir, basename + '.changelog'), 'w') as f:
                f.write(changelog)
            f.close()
        
        ### create archive
        os.chdir(workdir)
        with tarfile.open(out_file, 'w:' + compression) as tar_handle:
            tar_handle.add(destinationpath, recursive=True, filter=reset)

        log.info(f"Created archive at {tar_handle.name}")
        tar_handle.close()    
                 
        if publish_url:
            try:
                run_cmd(["mkdir -p update-archives"], '/srv/obs/repos')
                run_cmd(["rm -f /srv/obs/repos/update-archives/*"], '.')
                run_cmd(["cp -av " + out_file + " .",], '/srv/obs/repos/update-archives/')
                run_cmd(["cp -av " + os.path.join(contentdir, basename + '.changelog') + " .",], '/srv/obs/repos/update-archives/')
            except CalledProcessError:
                log.error(f"Error while copying tarfile to repo")
                exit(1)

            try:            
                run_cmd(['/usr/bin/rsync -ar0 --timeout 7200 /srv/obs/repos/update-archives/ ' + publish_url + '/'], '.')
            except CalledProcessError:
                log.error(f"Error while syncing files to {publish_url}")
                exit(1)
        
    except Exception as e:
        log.error(f"Error while trying to create archive: {e}")
        exit(1)


if __name__ == "__main__":
    main()
